Data Engineer Associate Progress

AWS S3 - Storage
Buckets ( container for storage ) and objects ( files )
Each bucket is created in a specific region

Rules - Buckets must have a globally unique name ( across all region , across all accounts )
length is b/w 3 to 63
only lowercase letters, no., dots, & hyphens

Key - Each object is identified by a unique, user-assigned key

Use cases - Backup & Recovery
Website, Applications
Data Archiving
Data Lakes


after creating s3 bucket we have some concepts for ingestion

In data ingestion, you're simply bringing data from one or more sources into a storage or processing system â€” like a data warehouse, data lake, or analytics platform.

âœ¨ Think of it as â€œfeedingâ€ your system with data!

âœ… Sources:

    Databases (SQL, NoSQL)
    APIs
    Files (CSV, JSON, Parquet)
    Streaming platforms (Kafka, Kinesis)
    IoT devices or logs

âœ… Destinations:

    Data lakes (e.g., S3, Azure Data Lake)
    Data warehouses (e.g., Redshift, BigQuery)
    Processing engines (e.g., Spark, Flink)

Batch ingestion: Data is collected and loaded in chunks at scheduled intervals.
Streaming ingestion: Data is continuously collected and processed in real-time or near real-time.

-- Streaming Ingestion
Enables real-time Ingestions
Ideal for time sensitive data
more expensive & intricate
implemented using services like Amazon kinesis for streaming data.

--- Batch Ingestion-
ingests data periodically in batches
typically large volumes
cost effective & efficient
tools like AWS GLUE commonly used

AWS GLUE:
AWS Glue is a fully managed ETL (Extract, Transform, Load) service by Amazon Web Services. It helps you prepare, transform, and move data between different data storesâ€”without worrying about managing servers or infrastructure.

Fully managed ETL (Extract, Transform, Load) Service
Desgined to make it easy to load & transform data
Visual Interface: Easily create ETL jobs without code

Various Integration -> S3, RDS, Redshift, DynamoDB, Kinesis Data Streams, Document DB etc.

Scripts auto generated behind the scenes'
Uses Spark behing the scenes ( Without need to manange anything )

ðŸ”¥ Scenario:
You have a CSV file with sales data stored in an S3 bucket. Weâ€™ll use AWS Glue to create a Data Catalog table so you can query this data with Athena.

âœ… Sales CSV file (stored in S3)
s3://my-bucket/sales-data/sales.csv

order_id,product,category,amount,date
1001,Laptop,Electronics,1200,2024-02-10
1002,Phone,Electronics,800,2024-02-11
1003,Shoes,Apparel,150,2024-02-12

















