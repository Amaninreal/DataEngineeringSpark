Data Engineer Associate Progress

AWS S3 - Storage
Buckets ( container for storage ) and objects ( files )
Each bucket is created in a specific region

Rules - Buckets must have a globally unique name ( across all region , across all accounts )
length is b/w 3 to 63
only lowercase letters, no., dots, & hyphens

Key - Each object is identified by a unique, user-assigned key

Use cases - Backup & Recovery
Website, Applications
Data Archiving
Data Lakes


after creating s3 bucket we have some concepts for ingestion

In data ingestion, you're simply bringing data from one or more sources into a storage or processing system â€” like a data warehouse, data lake, or analytics platform.

âœ¨ Think of it as â€œfeedingâ€ your system with data!

âœ… Sources:

    Databases (SQL, NoSQL)
    APIs
    Files (CSV, JSON, Parquet)
    Streaming platforms (Kafka, Kinesis)
    IoT devices or logs

âœ… Destinations:

    Data lakes (e.g., S3, Azure Data Lake)
    Data warehouses (e.g., Redshift, BigQuery)
    Processing engines (e.g., Spark, Flink)

Batch ingestion: Data is collected and loaded in chunks at scheduled intervals.
Streaming ingestion: Data is continuously collected and processed in real-time or near real-time.

-- Streaming Ingestion
Enables real-time Ingestions
Ideal for time sensitive data
more expensive & intricate
implemented using services like Amazon kinesis for streaming data.

--- Batch Ingestion-
ingests data periodically in batches
typically large volumes
cost effective & efficient
tools like AWS GLUE commonly used

AWS GLUE:
AWS Glue is a fully managed ETL (Extract, Transform, Load) service by Amazon Web Services. It helps you prepare, transform, and move data between different data storesâ€”without worrying about managing servers or infrastructure.

Fully managed ETL (Extract, Transform, Load) Service
Desgined to make it easy to load & transform data
Visual Interface: Easily create ETL jobs without code

Various Integration -> S3, RDS, Redshift, DynamoDB, Kinesis Data Streams, Document DB etc.

Scripts auto generated behind the scenes'
Uses Spark behing the scenes ( Without need to manange anything )

ðŸ”¥ Scenario:
You have a CSV file with sales data stored in an S3 bucket. Weâ€™ll use AWS Glue to create a Data Catalog table so you can query this data with Athena.

âœ… Sales CSV file (stored in S3)
s3://my-bucket/sales-data/sales.csv

order_id,product,category,amount,date
1001,Laptop,Electronics,1200,2024-02-10
1002,Phone,Electronics,800,2024-02-11
1003,Shoes,Apparel,150,2024-02-12

AWS Glue Data Catalog
Centralized Data Catalog:
Stores table schemas & metadata

allows querying by:
AWS Athena, RedShift, Quicksight, ENR


Glue Crawlers:
scan data sources,
infer schema,
stored in the AWS Glue Data Catalog
=> Can automatically classify data

Scheduling
Run on a schedule or based on triggers
increment loads/crawling (ETL Jobs & Crawlers)

âœ… Why do we need crawlers in AWS Glue?

A crawler is a service that scans your data stores (like S3, RDS, or DynamoDB), extracts metadata, and creates or updates tables in the AWS Glue Data Catalog.

    No manual schema definitions needed!
    It helps Glue understand your dataâ€™s structure for ETL (Extract, Transform, Load) jobs.
    Enables seamless querying using Amazon Athena, Redshift Spectrum, or EMR.

ðŸš€ Use cases for Glue Crawlers:

1ï¸âƒ£ Data Lake automation:

    Continuously discover and catalog data in S3 buckets, even as new files are added.
    Keep your Glue Data Catalog up-to-date without manual intervention.

2ï¸âƒ£ Schema evolution:

    Automatically detect schema changes (e.g., new columns) and update tables.
    Helps with semi-structured data like JSON, Parquet, or Avro.

3ï¸âƒ£ ETL pipelines:

    Simplify the initial data exploration step before building ETL jobs.
    Use the cataloged metadata in Glue ETL scripts or Athena queries.

4ï¸âƒ£ Data migration:

    When migrating from on-premise to cloud, crawlers help rebuild schema mappings quickly.



AWS Athena:

Interactive query service:
    Query files in s3 using SQL

Serverless:
    No infra to manage
    Pay as you go

S3 bucket -> crawler -> data catalog -> Athena -> quicksight

Athena we can use there as a data source in quicksight
QuickSight?

Amazon QuickSight (Visualization Layer)

    QuickSight connects to Athena to create interactive dashboards and reports.
    It visualizes your queried data, providing business insights in the form of charts, graphs, and KPIs.

âœ¨ Why? Turns raw data into actionable insights for decision-makers.

AWS Athena use cases
Log analysis
Analyzing log files stored in Amazon S3

Ad-Hoc Analysis:
Ad-hoc queries on data lakes stored in s3

Data Lake Analytics:
Building a data lake on amazon s3 & using athena to query data

Real time analytics:
Integrating athena with streaming data sources such as Amazon Kinesis

Athena - Federated Queries

Query data sources other than S3 buckets using a data connector

By this we can run queries in:
Relational & non - relational data sources
Object data sources
Custom data sources

Federated data sources: ( built in examples )

Amazon Cloud watch logs
Amazon dynamoDB
Amazon DocumentDB
Amazon RDS

work with multiple sources

Amazon RDS - Products table 
Amazon DocumentDB - detailed customer profile data
Amazon DynamoDb - user interactions

Athena - Cost

Pay for queries run - amount of data scanned
Cost saving possible with reserved capacity

Atena - Performance Optimization

Use Partitions - 
    it means we structure our data into partisions
    Eliminate data partitions that need to be scanned (pruning)

Use Partition Projection
    Automate Partion management
    Speed up queries for tables with large partitions

AWS Glue Partition Indexes:
    Athena retrieves only relevant partitions instead of loading all
    Optimize query planning & reduce query 

Athena - Query Result Reuse
What it does?
Reuse previous results that match your query & max. age

When to use?
Query where the source data doesn't change frequently.
Repeated queries
Large datasets the complex queries

Athena - performance Optimization
Data Compressions
Reduce file size to speed up queries

Format Conversion:
transform data into an optimized structure such as Apache Parquet or Apache ORC columnar formats

Athena Work groups:
Isolate queries from other queries in the same account

Isolate queries for different:
    Teams
    Use Cases
    Applications
    Different settings or to track & control cost

Control 
    Query execution settings
    Access
    Cost
    Type of engine ( Athena SQL vs Apache Spark)

upto 1000 workgroups per region
Each account has primary workgroup.














