Data Engineer Associate Progress

AWS S3 - Storage
Buckets ( container for storage ) and objects ( files )
Each bucket is created in a specific region

Rules - Buckets must have a globally unique name ( across all region , across all accounts )
length is b/w 3 to 63
only lowercase letters, no., dots, & hyphens

Key - Each object is identified by a unique, user-assigned key

Use cases - Backup & Recovery
Website, Applications
Data Archiving
Data Lakes


after creating s3 bucket we have some concepts for ingestion

In data ingestion, you're simply bringing data from one or more sources into a storage or processing system â€” like a data warehouse, data lake, or analytics platform.

âœ¨ Think of it as â€œfeedingâ€ your system with data!

âœ… Sources:

    Databases (SQL, NoSQL)
    APIs
    Files (CSV, JSON, Parquet)
    Streaming platforms (Kafka, Kinesis)
    IoT devices or logs

âœ… Destinations:

    Data lakes (e.g., S3, Azure Data Lake)
    Data warehouses (e.g., Redshift, BigQuery)
    Processing engines (e.g., Spark, Flink)

Batch ingestion: Data is collected and loaded in chunks at scheduled intervals.
Streaming ingestion: Data is continuously collected and processed in real-time or near real-time.

-- Streaming Ingestion
Enables real-time Ingestions
Ideal for time sensitive data
more expensive & intricate
implemented using services like Amazon kinesis for streaming data.

--- Batch Ingestion-
ingests data periodically in batches
typically large volumes
cost effective & efficient
tools like AWS GLUE commonly used

AWS GLUE:
AWS Glue is a fully managed ETL (Extract, Transform, Load) service by Amazon Web Services. It helps you prepare, transform, and move data between different data storesâ€”without worrying about managing servers or infrastructure.

Fully managed ETL (Extract, Transform, Load) Service
Desgined to make it easy to load & transform data
Visual Interface: Easily create ETL jobs without code

Various Integration -> S3, RDS, Redshift, DynamoDB, Kinesis Data Streams, Document DB etc.

Scripts auto generated behind the scenes'
Uses Spark behing the scenes ( Without need to manange anything )

ðŸ”¥ Scenario:
You have a CSV file with sales data stored in an S3 bucket. Weâ€™ll use AWS Glue to create a Data Catalog table so you can query this data with Athena.

âœ… Sales CSV file (stored in S3)
s3://my-bucket/sales-data/sales.csv

order_id,product,category,amount,date
1001,Laptop,Electronics,1200,2024-02-10
1002,Phone,Electronics,800,2024-02-11
1003,Shoes,Apparel,150,2024-02-12

AWS Glue Data Catalog
Centralized Data Catalog:
Stores table schemas & metadata

allows querying by:
AWS Athena, RedShift, Quicksight, ENR


Glue Crawlers:
scan data sources,
infer schema,
stored in the AWS Glue Data Catalog
=> Can automatically classify data

Scheduling
Run on a schedule or based on triggers
increment loads/crawling (ETL Jobs & Crawlers)

âœ… Why do we need crawlers in AWS Glue?

A crawler is a service that scans your data stores (like S3, RDS, or DynamoDB), extracts metadata, and creates or updates tables in the AWS Glue Data Catalog.

    No manual schema definitions needed!
    It helps Glue understand your dataâ€™s structure for ETL (Extract, Transform, Load) jobs.
    Enables seamless querying using Amazon Athena, Redshift Spectrum, or EMR.

ðŸš€ Use cases for Glue Crawlers:

1ï¸âƒ£ Data Lake automation:

    Continuously discover and catalog data in S3 buckets, even as new files are added.
    Keep your Glue Data Catalog up-to-date without manual intervention.

2ï¸âƒ£ Schema evolution:

    Automatically detect schema changes (e.g., new columns) and update tables.
    Helps with semi-structured data like JSON, Parquet, or Avro.

3ï¸âƒ£ ETL pipelines:

    Simplify the initial data exploration step before building ETL jobs.
    Use the cataloged metadata in Glue ETL scripts or Athena queries.

4ï¸âƒ£ Data migration:

    When migrating from on-premise to cloud, crawlers help rebuild schema mappings quickly.



AWS Athena:

Interactive query service:
    Query files in s3 using SQL

Serverless:
    No infra to manage
    Pay as you go

S3 bucket -> crawler -> data catalog -> Athena -> quicksight

Athena we can use there as a data source in quicksight
QuickSight?

Amazon QuickSight (Visualization Layer)

    QuickSight connects to Athena to create interactive dashboards and reports.
    It visualizes your queried data, providing business insights in the form of charts, graphs, and KPIs.

âœ¨ Why? Turns raw data into actionable insights for decision-makers.

















